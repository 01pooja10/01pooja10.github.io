## Blog Post Title From First Header

Autoencoders (AEs) are a family of generative models which make use of the encoder-decoder architecture to model data and reconstruct varied forms or representations of the input distribution. This aspect propels the autoencoder family into the generative AI sphere.

For those curious to learn more about what generative AI is, I penned down a gripping introductory blog which can be found hereğŸ¤“

The encoding mechanism enables an autoencoder to compress the original data and its dimensions to a latent space. The lower dimensional space acts as a bottleneck to later aid the decoder in reconstructing the data.

ğŸ’¡It is interesting to note that autoencoders employ unsupervised learning and search for structure in the data via compressed representation to make sense of their inputs.
---

### Some Math

Letâ€™s look at the mathematical notations involved:

x is the input image of number 4 (from the MNIST dataset) represented as a matrix of pixel values.
The encoder function gÏ†() compresses the input x (reduces its dimensionality) to form the compact bottleneck z.
The bottleneck z = gÏ†(x) helps the network learn the input structure instead of overfitting by merely memorizing the input data for reconstruction. Nonetheless, autoencoders possess this drawback.
The decoder function fÎ¸() reconstructs image data (or the number 4 in our case) resembling the input, from the latent representation. It outputs xâ€™ = fÎ¸(z).

ğŸ’¡ The low dimensional representation in the bottleneck is also referred to as latent code.

The loss function here is simply the measurement of error (or difference) between the original input image x and the reconstructed output image xâ€™ represented by fÎ¸(gÏ†(x)).

Loss(Î¸, Ï†) = 1/n . Î£ [x_i â€” fÎ¸(gÏ†(x_i))]Â²

The parameters Ï† and Î¸ are learnable through backpropagation during training. They help our network produce outputs similar to the input. But beware, as already mentioned above in point 3, sometimes the network may end up memorizing the input data points due to overfitting towards the identity function. Thus it will get stuck reproducing images not very unlike the input training data sample.

### Denoising Autoencoders (DAEs)ğŸ”‡
Since overfitting to the identity function is a fairly well-known drawback of autoencoders, denoising autoencoders (DAEs) were introduced to rectify this issue. How is a DAE different? It corrupts the input data (noises up the network) by canceling out certain input values sporadically.

So, we simply set the values of a few input dimensions to 0. This forces the network to â€œdenoiseâ€ the corrupted inputs. Hence the reconstructed data is similar to the original inputs but the model does not overfit.

This is because DAEs learn from a corrupted (partially visible input) distribution as compared to merely memorizing the input down to a tee. This indicates successful data retrieval and reconstruction from the latent distribution.

ğŸ’¡The loss function must compare the networkâ€™s outputs only with the original uncorrupted inputs.

### Add a V to the AE â€” Variational Autoencoders (VAEs)
Autoencoders use fixed vectors to map attributes from the input image to the latent space. This cripples the networkâ€™s ability to better learn, customize, or tweak certain input attributes.

For example, in our input image say we have a woman with blue hair highlights. A simple autoencoder maps a fixed vector (a single value) to the â€œblue hairâ€ attribute. It can probably only turn the blue hair on or off. But this is not the best way to quantify a transient attribute such as hair color.

On the other hand, variational autoencoders map a probability distribution (a range of values) for better control over the networkâ€™s latent representations as compared to one rigid latent vector in autoencoders for representing complex attributes.

Imagine we are comparing an on/off switch with a rotating dial that you can twist for better control. VAEs resemble the more controllable and flexible dial. This also helps take into consideration the nuances in our data.

The VAE consists of generative and inference models for the tasks of compressing and reconstructing data. Let x be our observed variable (or input) and z be our latent variable. Our goal is to infer the latent variable from our input. For that we need to determine the conditional probability distribution of the latent variable given the input i.e. we need to find pÎ¸(z|x).

#### Why variational inference?ğŸ¤”
Here, while computing the probability distribution pÎ¸(z|x) for the input, we use the Bayes rule to expand:

Equation A â†’ pÎ¸(z|x) = {pÎ¸(x|z) . pÎ¸(z)} Ã· {pÎ¸(x)}

Here,

pÎ¸(z|x) is the posterior
pÎ¸(x|z) is the likelihood
pÎ¸(z) is the prior and
pÎ¸(x) is the marginal
We need to compute the marginal pÎ¸(x) but this is arduous since it integrates over all the values of z:

pÎ¸(x) = âˆ« pÎ¸(x | z) . pÎ¸(z) dz

This computation is intractable (no efficient solution exists) in higher dimensional spaces as z being a latent space can have numerous values and dimensions. It requires exponential time for solving which is not plausible. Thus we resort to variational inference.

#### Variational inferenceğŸ˜ƒ
By making use of another known and tractable yet simple Gaussian distribution qÏ†(z|x), we approximate pÎ¸(x|z). So we need to ensure that pÎ¸(z|x) is modeled from qÏ†(z|x) by converting this into an optimization problem.

We can achieve this with the Kullback-Leibler (KL) divergence measure. KL divergence tells us how different the two probability distributions are. Letâ€™s take a look at the equation for KL divergence for a discrete random variable and two distributions say A and B.

KL (A||B) = Î£ A(xi) . log(A(xi) Ã· B(xi))

ğŸ’¡KL(A||B) â‰  KL(B||A) â€” the forward and reverse KL divergence measures are not equal and it is asymmetric in nature.

The KL divergence measure is non-negative and if the 2 distributions are mostly equal throughout, it amounts to zero. So for our 2 distributions p and q, we get:

KL(qÏ†(z|x)||pÎ¸(z|x)) = Î£ qÏ†(z|x) . log (qÏ†(z|x) Ã· pÎ¸(z|x))

### Loss function derivationğŸª„
In terms of the expectation and after some logarithmic simplification, the RHS becomes:

Equation B â†’ KL = E_z [ log (qÏ†(z|x)) â€” log( PÎ¸(z|x))]

In equation B,

1. KL = KL( qÏ†(z|x) || pÎ¸(z|x) )

2. E_z signifies the expectation over z wherein z is being sampled from qÏ†(z|x) and can also be written as â†’ z ~ qÏ†(z|x).

Substituting equation A in equation B, we get:

â†’ KL = E_z [ log (qÏ†(z|x)) â€” log( {pÎ¸(x|z) . pÎ¸(z)} Ã· {pÎ¸(x)} ) ]

â†’ KL = E_z [ log (qÏ†(z|x)) â€” log(pÎ¸(x|z)) â€” log(pÎ¸(z)) + log(pÎ¸(x)) ]

Although we still donâ€™t know pÎ¸(x), note that the expectation term only computes over z and doesnâ€™t involve x. So we can move the term log(pÎ¸(x)) to the LHS instead of retaining it in the RHS.

â†’ KL â€” log(pÎ¸(x)) = E_z [ log (qÏ†(z|x)) â€” log(pÎ¸(x|z)) â€” log (pÎ¸(z)) ]

Taking a minus sign on both sides and rearranging, we get,

â†’ log(pÎ¸(x)) â€” KL = E_z[ log(pÎ¸(x|z)) ] â€” E_z [ log (qÏ†(z|x)) â€” log (pÎ¸(z)) ]

â†’ log(pÎ¸(x)) â€” KL = E_z[ log(pÎ¸(x|z)) ] â€” E_z [ log ( qÏ†(z|x) Ã· pÎ¸(z) ) ]

Note that the second term (bolded) on the RHS is another KL divergence measure. Hence, we get:

Equation C â†’ log(pÎ¸(x)) â€” KL = E_z[ log(pÎ¸(x|z)) ] â€” KL ( qÏ†(z|x) || (pÎ¸(z) )

To arrive at the final loss function of a VAE, we attach a negative sign to the RHS of equation C:

L(Î¸, Ï†) = -E_z [log (pÎ¸(x|z))]+ KL[ qÏ†(z|x) || pÎ¸(z) ]

Also, the LHS of equation C can be represented as:

â†’ log(pÎ¸(x)) â€” KL [ qÏ†(z|x) || pÎ¸(z|x) ] = â€” L(Î¸, Ï†)

In the final loss function, the roles of both terms are as follows:

The first term is the reconstruction loss.
The second term is considered a regularizer term.


#### The Reparameterization trickğŸ˜
As we know, the expectation term E_z represents z being sampled from qÏ†(z|x) and is written as z ~ qÏ†(z|x). Unfortunately, the sampling of z is a stochastic process, so we are dealing with a random variable. Random variables donâ€™t allow us to calculate their derivatives and hence arenâ€™t backpropagation-friendly.

This is a problem because we canâ€™t train our network with random variables that canâ€™t be backpropagated. Hence we use the reparameterization trick: represent the random variable as a deterministic variable through a transformation function gÏ†().

z = gÏ† (Îµ, x) and z = Âµ + Ïƒ.Îµ

Here, Îµ is an extra stochastic random variable that allows z to become learnable due to the presence of mean Âµ and variance Ïƒ parameters, hence making it training-friendlyğŸ¤©

With this, we can better comprehend what goes on behind the scenes in a VAE. Whatâ€™s left is to build this model using a deep learning framework, train it, and later use it for inference purposes. For this purpose, refer to my GitHub code repository below.

You can find my PyTorch-based VAE implementation here:

### Vector Quantized VAEs (VQ-VAEs)
A VQ-VAE employs vector quantized representations of latent spaces and its encoder outputs â€œdiscreteâ€ codes to model and then the input data is compressed. Further, the prior distribution is learned and not fixed.

The authors of the VQ-VAE paper decided to incorporate discrete representations as they more accurately resemble naturally available data such as language which is a sequence of characters and images that can be described by words, etc. Through this method, drawbacks such as posterior collapse and exploding variance are alleviated.

The image above depicts how data flows through the entire network(both forward and backward). We have a convolutional neural network-based (CNN) encoder and decoder. Additionally, we have the vector quantization block between the encoder and decoder which uses an embedding space with embedding vectors e_i âˆˆ â„ where i âˆˆ 1,2,...K.

#### Forward propagationâ¡ï¸
Encoder: It uses a CNN to encode the input data (x) and gives out: z_e(x) of size D which is the embedding size.

We then calculate the one-hot posterior q(z|x) distribution. Discrete latent variables z are found using nearest neighbor lookup to match with one of k embedding vectors from the embedding space.

ğŸ’¡k is assigned according to the L2 norm calculated between the encoderâ€™s output and embedding vector. This helps ascertain the embedding element nearest to z_e(x) i.e. the least distance between them.

- The term q(z|x) is indexed using the codebook to extract embedding vectors where integer k is the common link connecting the z_e(x) to z_q(x).

- We then find the embedding vector (represented as e_k) nearest to z_e(x) for ensuring dimensional uniformity and assign it to z_q(x).

- Now, z_q(x) is passed on to the decoder network for image reconstruction.

### Backpropagation: problems and solutionâ¬…ï¸

The codebook vector is not differentiable â€” gradients canâ€™t be propagated back through argmin as it maps k to discrete integers (indices of the closest vectors). Thus we use a straight-through estimation mechanism that copies the gradients of z_q(x) into z_e(x) since both vectors have the same dimensions. This is represented as a red line in the VQ-VAE: Working Mechanism image above to indicate straight-through gradient estimation. So now, we can go backğŸ˜‰

Loss function
Here we have the final training loss or objective:


We have 3 terms, all separated by plus (+) signs in the equation above. Here, sg is the stop gradient constraint or operator which signifies the absence of any gradients for the term it encompasses and uses a non-upgradable constant. So letâ€™s break the equation down and tackle each term.

- First term log p(x|z_q(x)) â†’ The reconstruction loss optimizes both the encoder and decoder networks.
- Second term ||sg[z_e(x)] â€” e||2 2 â†’ The Vector Quantization (VQ) loss is also called the codebook objective. The L2 error is used to move the embedding vector e_i closer to the encoded outputs z_e(x). This term helps learn and update the embedding space.
- Third term Î²||z_e(x)âˆ’sg[e]||2 2 â†’ The commitment loss function ensures that the encoderâ€™s outputs donâ€™t vacillate or grow too much and stay close to the embedding space.

ğŸ’¡VQ-VAEs achieve commendable likelihood for generating 128x128 color images, speech, and video action sequences too!

You can find an example of a VQ-VAEâ€™s performance with 32x32x1 latent space and k=512 attached as an image below. This has been obtained from the original research paper by Oord et. al. and can be found here. The left half consists of original images from ImageNet and the right half shows the modelâ€™s reconstructed versions of the same.

In this blog, we have explored, analyzed, and understood how autoencoders have been evolving with new additions and conceptual improvements that enhance their generative capabilities.

### ConclusionğŸ˜„
OK, you have successfully gotten through this detailed and hopefully informative blog! Whatâ€™s next? I will be releasing the third blog in this series soon. It will be all about Generative Adversarial Networks (GANs) â€” the concepts used to build them, some useful math to understand their working, etc. Stay tuned!ğŸ«¡

Until then, if you have any doubts/suggestions, or would simply like to chat, feel free to reach out to me via LinkedIn.

